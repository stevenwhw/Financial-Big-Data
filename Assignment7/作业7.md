<h1><center>作业7</center></h1>
### 1.简述为什么会有Spark

  	（1）由于Hadoop计算框架对很多非批处理大数据问题的局限性，除了原有的基于Hadoop Hbase的数据存储管理模式和Mapreduce计算模式外，人们开始关注大数据处理所需要的其他各种计算模式和系统。

​	  （2）后Hadoop时代新的大数据计算模式和系统的出现，其中尤其以内存计算为核心、集诸多计算模式之大成的Spark生态系统的出现为典型代表。大数据不仅仅需要大数据查询分析计算和批处理计算，还需要流式计算、迭代计算、图计算和内存计算。

​	  （3）Spark基于内存计算思想提高计算性能，速度快、易用性好、通用性好、支持多种模式运行。

### 2. 对比Hadoop和Spark

（1）Spark把中间数据放在内存中，迭代运算效率高，而MapReduce中计算结果需要落地，保存到磁盘上。Spark支持DAG图的分布式并行计算，减少了迭代过程中数据的落地，提高了处理效率。

（2）Spark容错性高。Spark引入了RDD的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的。在RDD计算是可以通过CheckPoint来实现容错，而CheckPoint有两种方式：CheckPoint Data和Logging The Updates。

（3）Spark更加通用。Hadoop只提供Map和Reduce两种操作，Spark提供的数据集操作类型很多。另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种，用户可以命名、物化、控制中间结果的存储、分区等。

（4）Spark只分布式计算，而Hadoop分布式存储、分布式计算

（5）Spark中是Generalized computation，而Hadoop是MapReduce结构

（6）Spark中的数据在磁盘或者内存中，而Hadoop中通常在磁盘（HDFS）中

（7）Hadoop是批处理、而Spark更快，且Spark支持java、python、Scala语言编程

（8）生态对比

| Use case                                  | Hadoop                           | Spark                                                       |
| ----------------------------------------- | -------------------------------- | ----------------------------------------------------------- |
| Batch processing(批处理)                  | Hadoop的MapReduce(Java,Pig,Hive) | Spark RDDs(java,scala,python)                               |
| SQL querying                              | Hadoop:Hive                      | Spark SQL                                                   |
| Streaming Processing/Real Time processing | Storm,Kafka                      | Spark Streaming                                             |
| Machine Learning                          | Mahout                           | Spark ML Lib                                                |
| Real time                                 | NoSQL(Hbase,Cassandra ..etc)     | No Spark component But Spark can query data in NoSQL stores |

（9）优势对比

| Hadoop     | Spark             |
| ---------- | ----------------- |
| 无限的规模 | 易于开发          |
| 企业平台   | 内存性能好        |
| 应用范围广 | combine workflows |



### 3.简述Spark的技术特点

（1）RDD：Spark提出的弹性分布式数据集，是Spark最核心的分布式数据抽象，Spark的很多特性都和RDD密不可分。

（2）Transformation&Action：Spark通过RDD的两种不同类型的运算实现了惰性计算，即在RDD的Transformation运算时，Spark并没有进行作业的提交，而在RDD的Action操作时才会触发SparkContext提交作业。

（3）Lineage：为了保证RDD中数据的鲁棒性，Spark系统通过血统关系（lineage）来记录一个RDD是如何通过其他一个或者多个父类RDD转变过来的，当这个RDD的数据丢失时，Spark可以通过它父类的RDD重新计算。

（4）Spark调度：Spark采用了事件驱动的Scala库类Akka来完成任务的启动，通过服用线程池的方式来取代MapReduce进程或者线程启动和切换的开销。

（5）API：Spark使用Scala语言进行开发，并且默认Scala作为其编程语言。因此，编写Spark程序比MapReduce程序要简洁得多。同时，Spark系统也支持Java、Python语言进行开发

（6）Spark生态：Spark SQL、Spark Streaming、GraphX等为Spark的应用提供了丰富的场景和模型，适合应用于不同的计算模式和计算任务

（7）Spark部署：Spark拥有Standalone、Mesos、YARN等多种部署方式，可以部署在多种底层平台上

（8）适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小

（9）由于RDD的特性，Spark不适用那些异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合

（10）数据量不是特别大，但是要求实时统计分析需求

综上所述，Spark是一个基于内存的迭代式、关系查询、流式处理等计算密集型任务。